% Based on:
% https://v1.overleaf.com/latex/templates/vcu-math-490-review-of-an-interesting-article/smwgtrztrnsn.pdf
% and https://www.overleaf.com/latex/templates/sbc-conferences-template/blbxwjwzdngr

\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

%\usepackage[brazil]{babel}   
\usepackage[utf8]{inputenc}  

\sloppy

\title{The Design and Implementation of a Log-Structured File System Review}

\author{Thiago A. Sposito\inst{1} }


\address{Departamento de Ciência da Computacão Universidade Federal de Minas Gerais (UFMG)}

\begin{document} 

\maketitle


\section{Introduction}
The Design and Implementation of a Log-Structured File
System is an article from 1992. Mendel Rosenblum (VMWare founder) and John Ousterhout (TCL/Tk creator) are the authors. This paper proposes a novel approach based on writing log-like long strands of data at once to maximize hardware write bandwidth and discuss its drawbacks and proposed mitigation methods.

\section{Summary} 
After the introduction in Section 1, the authors discuss the current approach in file systems back in the time. Section 3 present the log-structured file system, its advantages and potential pitfalls. The following Section discusses crash recovery in this file system; section 5 is about benchmarks and performance analysis. Section 6 will discuss related work from where LFS borrowed ideas. Furthermore, finally, we have the conclusion section.


\section{Analysis}
The LFS is based on the premise that hard drive speeds would grow slower than the CPU speeds and memory capacity\cite{walter2005kryder}. In particular, about memory, the authors predicted that writes would dominate disk traffic with increased real estate for caching. The notion of logs was not new, as it was already a common technique used to speed up crash recovery; the novelty here is using it as a permanent storage format in the hard drive.
In order to take maximum advantage of disk writes, this file system needs access to large, not fragmented extensions on the disk. Here it uses a couple of techniques for bookkeeping using a cleaver algorithm capable of treating data with different levels of ephemerality with distinct policies when performing data compression and reallocation in the cleaning phase.

Those policies are decided based on the write cost, which is calculated by the reason of the total amount of bytes read and written by the new amount of data in bytes to be written. In order to achieve the highest performance, the algorithm will try to make most segments nearly full, giving a few empty or nearly empty segments to the cleaner work.

\section{Conclusion}
In a nutshell, LFS works on the premise of getting large amounts of that cached and later perming a single large I/O operation to get the most hardware throughput\cite{rosenblum1992design}. That is, although easier said than done, cleaver and somewhat complicated algorithms were needed in order to make this viable, taking faster processors and memory to its advantage. This was a pretty neat idea, but maybe too radical by the time as it was a profound shift in file system paradigm\cite{tweedie1998journaling}. 


\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
